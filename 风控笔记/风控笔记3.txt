CART树：基础单棵回归/分类树，用Gini/方差分裂，易过拟合，起点。决策树容易过拟合。
AdaBoost：串行提升，前向加权弱树（常CART），错分样本权重↑，从“民主投票”进化到关注难点。训练的时候不仅预测错误的样本的权重要上升，假如这个树预测的准确率低那么这棵树训练完后他自身的权重也要降低，两个是一个交叉的。∑ α_t × h_t(x)
GBDT：梯度提升，残差拟合，用梯度负方向建树，比AdaBoost更通用/稳定，从权重→梯度。依旧是每棵树都是学习所有的样本，但是后面的树只学习错的，假如后四个第一棵树预测错了，可能就是不是学 1 0 1 0 1 1 1 0 1 0 现在就是学 0 0 0 0 0 0 -1 0 -1 0这样。最后新的样本过来，结果就是第一个样本的预测+后面的所有。F0 + ∑ f_t(x)
XGBoost：GBDT增强版，二阶泰勒展开+正则（树复杂度/叶子权重）+列采样+稀疏感知，并行/可扩展，工程优化王者。
LightGBM：XGBoost轻量版，直方图算法（可以减少gini的计算）+叶节点分裂（Leaf-wise 不像xgboost那样子限定高度而是可能根据信息增益有一条很长）+类别特征优化+EFB捆绑，内存/速度碾压，极致高效。